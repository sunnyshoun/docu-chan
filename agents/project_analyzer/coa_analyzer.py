"""
CoA Project Analyzer - Chain of Agents æ¶æ§‹çš„å°ˆæ¡ˆåˆ†æå™¨

ä½¿ç”¨ CoA æ¶æ§‹æå‡åˆ†ææº–ç¢ºåº¦èˆ‡é€Ÿåº¦ï¼š
1. Worker Agents: å¹³è¡Œè™•ç†å„æª”æ¡ˆï¼Œç”¢ç”Ÿ local summary
2. Manager Agent: æ•´åˆæ‰€æœ‰ worker çµæœï¼Œå„ªåŒ–æœ€çµ‚å ±å‘Š
3. æ”¯æ´ async å¹³è¡Œè™•ç†ï¼Œå¤§å¹…æå‡å¤šæª”æ¡ˆåˆ†æé€Ÿåº¦
4. è‡ªå‹•è™•ç† .gitignore æ’é™¤è¦å‰‡
5. Worker timeout é˜²æ­¢å¡ä½
"""
import json
import asyncio
import traceback
from typing import Dict, Any, List
from datetime import datetime
from pathlib import Path

from config.agents import AgentName
from agents.base import BaseAgent
from agents.project_analyzer.models import FileInfo, FileAnalysisResult, ProjectContext
from agents.project_analyzer.scanner import GitIgnoreParser, FileScanner
from agents.project_analyzer.file_worker import FileAnalyzerWorker, WORKER_TIMEOUT, LINES_PER_READ
from agents.project_analyzer.manager import AnalysisManager
from agents.project_analyzer.image_worker import ImageWorker
from utils import file_utils
from utils.logger import get_logger
from utils.cli_ui import get_ui_manager
from tools.file_ops import set_project_root, clear_reports


class CoAProjectAnalyzer(BaseAgent):
    """
    CoA å°ˆæ¡ˆåˆ†æå™¨ - ä½¿ç”¨ Chain of Agents æ¶æ§‹
    
    ç‰¹é»ï¼š
    1. è‡ªå‹•è™•ç† .gitignore æ’é™¤è¦å‰‡
    2. Worker å¸¶ timeout é˜²æ­¢å¡ä½
    3. ä¸»æµ Agent æª”æ¡ˆè®€å–æ¶æ§‹ï¼ˆä¸ä¸€å€‹ä¸€å€‹è©¢å•ï¼‰
    4. å¹³è¡Œè™•ç†æå‡é€Ÿåº¦
    """
    
    def __init__(
        self, 
        root_dir: str, 
        prompt_dir: str, 
        dump_file: str, 
        report_file: str,
        max_parallel: int = 5,
        worker_timeout: float = WORKER_TIMEOUT
    ) -> None:
        super().__init__(
            agent_name=AgentName.FILE_WORKER,
            display_name="CoAProjectAnalyzer"
        )
        
        self.root_dir = Path(root_dir).resolve()
        self.dump_file = dump_file
        self.report_file = report_file
        self.max_parallel = max_parallel
        self.worker_timeout = worker_timeout
        self.dumps: List[Dict] = []
        self.report: Dict[str, Dict] = {}
        self.metadata: Dict[str, Any] = {}
        
        # è¨­å®š tools æ ¹ç›®éŒ„
        set_project_root(str(self.root_dir))
        
        # åˆå§‹åŒ– GitIgnore è§£æå™¨
        self.gitignore = GitIgnoreParser(self.root_dir)
        
        # æƒææª”æ¡ˆï¼ˆå°Šé‡ gitignoreï¼‰
        scanner = FileScanner(self.root_dir, self.gitignore)
        self.files = scanner.scan()
        
        if not self.files:
            raise FileNotFoundError(f"No files found in: {root_dir}")
        
        # ç‚ºäº†ç›¸å®¹æ€§
        self.file_nodes = self.files
        self.file_paths = [f.path for f in self.files]
        
        # å»ºç«‹æª”æ¡ˆæ¨¹å­—ä¸²
        self.file_tree_str = self._build_file_tree_str()
        
        # åˆå§‹åŒ–åœ–ç‰‡åˆ†æ Worker
        try:
            self.image_worker = ImageWorker(
                base=str(self.root_dir.parent),
                prompt_file=Path(prompt_dir) / "image_reader.json"
            )
        except Exception:
            self.image_worker = None
        
        # åˆå§‹åŒ– Manager
        self.manager = AnalysisManager()
        
        self.log(f"Initialized with {len(self.files)} files (ignored {len(self.gitignore.patterns)} patterns)")
    
    def _build_file_tree_str(self) -> str:
        """å»ºç«‹æª”æ¡ˆæ¨¹å­—ä¸²"""
        lines = [f"- {f.path}" for f in sorted(self.files, key=lambda x: x.path)[:100]]
        if len(self.files) > 100:
            lines.append(f"... and {len(self.files) - 100} more files")
        return '\n'.join(lines)
    
    def _identify_entry_points(self) -> List[str]:
        """è­˜åˆ¥å…¥å£æª”æ¡ˆ"""
        entry_patterns = {
            'main.py', 'app.py', 'cli.py', '__main__.py',
            'index.js', 'index.ts', 'main.js', 'main.ts',
            'server.py', 'server.js', 'setup.py', 'pyproject.toml', 'package.json'
        }
        return [f.path for f in self.files if f.abs_path.name.lower() in entry_patterns]
    
    def execute(self, project_path: str = "", **kwargs) -> Dict[str, Any]:
        """åŸ·è¡Œåˆ†æï¼ˆåŒæ­¥åŒ…è£ï¼‰"""
        asyncio.run(self.start_async())
        return {"report": self.report, "metadata": self.metadata}
    
    def start(self, max_retries: int = 3):
        """é–‹å§‹åˆ†æï¼ˆåŒæ­¥åŒ…è£ï¼‰"""
        asyncio.run(self.start_async(max_retries))
    
    async def start_async(self, max_retries: int = 3):
        """é–‹å§‹éåŒæ­¥åˆ†æ"""
        logger = get_logger()
        ui = get_ui_manager()
        clear_reports()
        
        start_time = datetime.now()
        logger.info(f"åˆ†æ {len(self.files)} å€‹æª”æ¡ˆ (parallel={self.max_parallel}, timeout={self.worker_timeout}s)")
        
        # å»ºç«‹å°ˆæ¡ˆä¸Šä¸‹æ–‡
        context = ProjectContext(
            root_dir=self.root_dir,
            files=self.files,
            file_tree_str=self.file_tree_str,
            gitignore_patterns=self.gitignore.patterns,
            entry_points=self._identify_entry_points()
        )
        
        # åˆ†çµ„è™•ç†æª”æ¡ˆ
        groups = self._group_files_by_directory()
        
        # ä½¿ç”¨ Phase 1 UIï¼ˆå•Ÿå‹•å¾Œä¸è¦ç”¨ printï¼‰
        ui.start_phase1(total_files=len(self.files))
        
        try:
            all_results: List[FileAnalysisResult] = []
            
            for group_name, group_files in groups.items():
                logger.debug(f"[CoAAnalyzer] è™•ç†åˆ†çµ„: {group_name} ({len(group_files)} files)")
                results = await self._process_group_async(group_files, context)
                all_results.extend(results)
            
            # å…ˆåœæ­¢ UI å†è¼¸å‡ºå¾ŒçºŒè¨Šæ¯
            ui.end_phase1()
            
            # Manager æ•´åˆ
            logger.info("ğŸ”¨ æ•´åˆåˆ†æçµæœ...")
            self.report, self.metadata = await self.manager.aggregate(all_results, context)
            
            # ä¿å­˜çµæœ
            self._save_progress()
            
            duration = (datetime.now() - start_time).total_seconds()
            logger.info(f"âœ… åˆ†æå®Œæˆï¼š{duration:.1f}s ({len(self.report)} files)")
            
        except Exception as e:
            logger.error(f"â— åˆ†æå¤±æ•—: {e}")
            logger.debug(f"Traceback:\n{traceback.format_exc()}")
            # ç¢ºä¿ UI è¢«åœæ­¢ï¼ˆéŒ¯èª¤æ™‚ï¼‰
            ui.end_phase1()
            raise
    
    def _group_files_by_directory(self) -> Dict[str, List[FileInfo]]:
        """æŒ‰ç›®éŒ„åˆ†çµ„æª”æ¡ˆ"""
        groups: Dict[str, List[FileInfo]] = {}
        for f in self.files:
            parts = Path(f.path).parts
            key = parts[0] if len(parts) > 1 else "_root"
            if key not in groups:
                groups[key] = []
            groups[key].append(f)
        return groups
    
    async def _process_group_async(
        self, files: List[FileInfo], context: ProjectContext
    ) -> List[FileAnalysisResult]:
        """å¹³è¡Œè™•ç†ä¸€çµ„æª”æ¡ˆ - ç°¡åŒ–é¡¯ç¤ºæ¨¡å¼ï¼Œåªé¡¯ç¤ºè®€å–è³‡è¨Š"""
        logger = get_logger()
        ui = get_ui_manager()
        semaphore = asyncio.Semaphore(self.max_parallel)
        
        # è¿½è¹¤ worker ç‹€æ…‹
        worker_counter = [0]  # ä½¿ç”¨ list è®“ closure å¯ä»¥ä¿®æ”¹
        total_files = len(files)
        completed = [0]
        
        logger.debug(f"[CoAAnalyzer] é–‹å§‹è™•ç† {total_files} å€‹æª”æ¡ˆ (parallel={self.max_parallel})")
        
        async def analyze_one(file_info: FileInfo, file_idx: int) -> FileAnalysisResult:
            logger.debug(f"[CoAAnalyzer] æ’éšŠç­‰å¾… semaphore: {file_info.path} (idx={file_idx})")
            async with semaphore:
                logger.debug(f"[CoAAnalyzer] å–å¾— semaphore: {file_info.path}")
                # åˆ†é… worker ID
                worker_counter[0] += 1
                worker_id = worker_counter[0] % self.max_parallel + 1
                
                # åˆ¤æ–· worker é¡å‹
                is_image = file_info.abs_path.suffix.lower() in FileScanner.IMAGE_EXTENSIONS
                
                # è¨ˆç®—ä¼°è¨ˆçš„è¡Œæ•¸
                try:
                    line_count = len(file_info.abs_path.read_text(encoding='utf-8', errors='replace').splitlines())
                    end_line = min(LINES_PER_READ, line_count)
                except:
                    line_count = 0
                    end_line = 0
                
                # ä½¿ç”¨æ–° API æ›´æ–° worker ç‹€æ…‹
                ui.update_file_worker(worker_id, file_info.path, (1, end_line), is_image)
                
                worker = FileAnalyzerWorker(
                    root_dir=self.root_dir,
                    image_worker=self.image_worker
                )
                
                logger.debug(f"[CoAAnalyzer] é–‹å§‹åˆ†æ: {file_info.path}")
                result = await worker.analyze(
                    file_info,
                    context=context.file_tree_str,
                    timeout=self.worker_timeout
                )
                logger.debug(f"[CoAAnalyzer] åˆ†æå®Œæˆ: {file_info.path} -> {result.summary[:50] if result.summary else 'N/A'}...")
                
                # æ›´æ–°å®Œæˆè¨ˆæ•¸ä¸¦æ¨™è¨˜ worker å®Œæˆ
                completed[0] += 1
                ui.file_worker_done(worker_id)
                
                # è¨˜éŒ„éŒ¯èª¤
                if result.error:
                    logger.warning(f"âš  {file_info.path}: {result.error}")
                    ui.add_error(f"{file_info.path}: {result.error}")
                
                self.dumps.append({
                    "time": str(datetime.now()),
                    "file": file_info.path,
                    "important": result.is_important,
                    "error": result.error
                })
                
                return result
        
        tasks = [analyze_one(f, i) for i, f in enumerate(files)]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        final_results = []
        for i, r in enumerate(results):
            if isinstance(r, Exception):
                logger.error(f"âŒ {files[i].path}: {r}")
                final_results.append(FileAnalysisResult(
                    file_path=files[i].path, is_important=False,
                    summary=f"[Error: {r}]", error=str(r)
                ))
            else:
                final_results.append(r)
        
        return final_results
    
    def _save_progress(self):
        """ä¿å­˜åˆ†æé€²åº¦"""
        file_utils.write_file(self.dump_file, json.dumps(self.dumps, indent=2, ensure_ascii=False))
        report_data = {"metadata": self.metadata, "files": self.report}
        file_utils.write_file(self.report_file, json.dumps(report_data, indent=2, ensure_ascii=False))


def create_coa_analyzer(
    root_dir: str,
    prompt_dir: str,
    dump_file: str,
    report_file: str,
    max_parallel: int = 5,
    worker_timeout: float = WORKER_TIMEOUT
) -> CoAProjectAnalyzer:
    """
    å»ºç«‹ CoA åˆ†æå™¨çš„å·¥å» å‡½æ•¸
    
    Args:
        root_dir: å°ˆæ¡ˆæ ¹ç›®éŒ„
        prompt_dir: Prompt ç›®éŒ„
        dump_file: Dump è¼¸å‡ºæª”æ¡ˆè·¯å¾‘
        report_file: å ±å‘Šè¼¸å‡ºæª”æ¡ˆè·¯å¾‘
        max_parallel: æœ€å¤§å¹³è¡Œ Worker æ•¸é‡
        worker_timeout: Worker è¶…æ™‚æ™‚é–“ï¼ˆç§’ï¼‰
    
    Returns:
        CoAProjectAnalyzer: é…ç½®å¥½çš„åˆ†æå™¨
    """
    return CoAProjectAnalyzer(
        root_dir=root_dir,
        prompt_dir=prompt_dir,
        dump_file=dump_file,
        report_file=report_file,
        max_parallel=max_parallel,
        worker_timeout=worker_timeout
    )
