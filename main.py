"""
Docu-chan - AI Documentation Generator

Usage:
    python main.py <project_path> [--request "..."]
    python main.py --chart "description"

ç°¡åŒ–æ¶æ§‹æµç¨‹ï¼š
- Phase 1: å°ˆæ¡ˆåˆ†æ (CoA) â†’ è¼¸å‡º report.json
- Phase 2: ä»»å‹™è¦åŠƒ (ç°¡åŒ–) â†’ è¼¸å‡º planner_output.json
- Phase 3: å…§å®¹ç”Ÿæˆ â†’ è¼¸å‡º docs/, charts/
"""
import sys
import json
import argparse
from pathlib import Path
from datetime import datetime
from typing import Optional, Dict, Any

from dotenv import load_dotenv

project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from config import config, load_config
from agents import (
    ProjectAnalyzer,
    DocPlanner,
    ChartLoop,
    DocWriter
)
from agents.doc_planner import PlannerOutput, DocTodo, ChartTodo
from utils.file_utils import ensure_dir, write_file, read_file
from utils.logger import setup_logger, get_logger, Operation


def create_session_id() -> str:
    """å»ºç«‹ session ID (timestamp)"""
    return datetime.now().strftime("%Y%m%d_%H%M%S")


def load_session_data(session_id: str, from_phase: int) -> Dict[str, Any]:
    """
    è¼‰å…¥ä¹‹å‰ session çš„è³‡æ–™
    
    Args:
        session_id: Session ID (e.g., "20251229_224459")
        from_phase: è¦å¾å“ªå€‹ phase é–‹å§‹ (1, 2, 3)
    
    Returns:
        dict: åŒ…å«éœ€è¦çš„ session è³‡æ–™
    
    Raises:
        FileNotFoundError: å¦‚æœå¿…è¦çš„æª”æ¡ˆä¸å­˜åœ¨
        ValueError: å¦‚æœ session ä¸å­˜åœ¨
    """
    session_dir = config.logs_dir / session_id
    if not session_dir.exists():
        raise ValueError(f"Session not found: {session_id}")
    
    data = {
        "session_dir": session_dir,
        "project_structure": None,
        "planner_output": None
    }
    
    # å¦‚æœè¦å¾ phase 2 æˆ–æ›´å¾Œé¢é–‹å§‹ï¼Œéœ€è¦ phase 1 çš„è³‡æ–™
    if from_phase >= 2:
        report_file = session_dir / "phase1" / "report.json"
        if not report_file.exists():
            raise FileNotFoundError(f"Phase 1 report not found: {report_file}")
        with open(report_file, "r", encoding="utf-8") as f:
            data["project_structure"] = json.load(f)
    
    # å¦‚æœè¦å¾ phase 3 é–‹å§‹ï¼Œéœ€è¦ phase 2 çš„è³‡æ–™
    if from_phase >= 3:
        planner_file = session_dir / "phase2" / "planner_output.json"
        if not planner_file.exists():
            raise FileNotFoundError(f"Phase 2 planner output not found: {planner_file}")
        with open(planner_file, "r", encoding="utf-8") as f:
            planner_data = json.load(f)
            # è½‰æ›ç‚º PlannerOutput ç‰©ä»¶
            doc_todos = [DocTodo.from_dict(t) for t in planner_data.get("doc_todos", [])]
            chart_todos = [ChartTodo.from_dict(t) for t in planner_data.get("chart_todos", [])]
            data["planner_output"] = PlannerOutput(
                doc_todos=doc_todos,
                chart_todos=chart_todos,
                project_summary=planner_data.get("project_summary", "")
            )
    
    return data


# ============================================================
# Phase Functions
# ============================================================

def run_analyzer(project_path: Path, session_dir: Path, max_parallel: int = 5) -> Dict[str, Any]:
    """
    Phase 1: åˆ†æå°ˆæ¡ˆï¼ˆä½¿ç”¨ CoA æ¶æ§‹ï¼‰
    
    Args:
        project_path: å°ˆæ¡ˆè·¯å¾‘
        session_dir: Session ç›®éŒ„
        max_parallel: æœ€å¤§å¹³è¡Œè™•ç†æ•¸
    
    Returns:
        project_structure: {
            "files": {path: {"summary": ..., "importance": ..., ...}},
            "metadata": {...}
        }
    
    Output Files:
        - phase1/report.json: å®Œæ•´åˆ†æå ±å‘Š
        - phase1/dump.json: åŸå§‹åˆ†æè³‡æ–™
    """
    logger = get_logger()
    logger.info("ğŸ“‚ Phase 1: åˆ†æå°ˆæ¡ˆ...")
    
    phase_dir = session_dir / "phase1"
    ensure_dir(phase_dir)
    
    report_file = phase_dir / "report.json"
    
    # ä½¿ç”¨ CoA æ¶æ§‹ï¼ˆWorker + Managerï¼‰
    analyzer = ProjectAnalyzer(
        root_dir=str(project_path),
        prompt_dir=str(config.prompts_dir / "project_analyzer"),
        dump_file=str(phase_dir / "dump.json"),
        report_file=str(report_file),
        max_parallel=max_parallel
    )
    
    # ç›´æ¥é–‹å§‹åˆ†æï¼ˆcoa_analyzer å…§éƒ¨æœƒé¡¯ç¤ºé€²åº¦ UI å’Œå®Œæˆè¨Šæ¯ï¼‰
    analyzer.start()
    
    # å¾æª”æ¡ˆè®€å–å ±å‘Šï¼ˆç¢ºä¿èˆ‡å„²å­˜çš„ä¸€è‡´ï¼‰
    with open(report_file, "r", encoding="utf-8") as f:
        report_data = json.load(f)
    
    # è¿”å›å®Œæ•´æ¶æ§‹ï¼ˆä¾› Planner ä½¿ç”¨ï¼‰
    return report_data


def run_planner(project_structure: Dict[str, Any], user_request: str, project_path: Path, session_dir: Path) -> PlannerOutput:
    """
    Phase 2: ä»»å‹™è¦åŠƒï¼ˆç°¡åŒ–ç‰ˆ - é è¨­åªè¦åŠƒ README + flow_chartï¼‰
    
    Args:
        project_structure: Phase 1 ç”¢ç”Ÿçš„æ¶æ§‹ JSON (report.json)
        user_request: ä½¿ç”¨è€…è«‹æ±‚
        project_path: å°ˆæ¡ˆè·¯å¾‘
        session_dir: Session ç›®éŒ„
    
    Returns:
        PlannerOutput: åŒ…å« doc_todos å’Œ chart_todos
    
    Output Files:
        - phase2/planner_output.json: è¦åŠƒçµæœ
    """
    logger = get_logger()
    logger.info("ğŸ“‹ Phase 2: ä»»å‹™è¦åŠƒ...")
    
    phase_dir = session_dir / "phase2"
    ensure_dir(phase_dir)
    
    # ä½¿ç”¨ DocPlanner
    planner = DocPlanner()
    output = planner.execute(
        user_request=user_request,
        report=project_structure,
        project_path=str(project_path)
    )
    
    # å„²å­˜è¦åŠƒçµæœ
    output_file = phase_dir / "planner_output.json"
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(output.to_dict(), f, indent=2, ensure_ascii=False)
    
    logger.info(f"   è¦åŠƒ {len(output.doc_todos)} ä»½æ–‡ä»¶, {len(output.chart_todos)} å¼µåœ–è¡¨")
    
    return output


def run_generator(
    planner_output: PlannerOutput,
    project_structure: Dict[str, Any],
    project_path: Path,
    session_dir: Path,
    session_id: str
) -> tuple[list, list]:
    """
    Phase 3: ç”Ÿæˆå…§å®¹ï¼ˆæ•´åˆè¨­è¨ˆ+ç”Ÿæˆï¼‰
    
    Args:
        planner_output: Phase 2 ç”¢ç”Ÿçš„ PlannerOutput
        project_structure: Phase 1 ç”¢ç”Ÿçš„æ¶æ§‹ JSON
        project_path: å°ˆæ¡ˆè·¯å¾‘
        session_dir: Session ç›®éŒ„
        session_id: Session ID
    
    Returns:
        (generated_charts, generated_docs)
    
    Output Files:
        - phase3/results.json: ç”Ÿæˆçµæœæ‘˜è¦
        - outputs/<session_id>/charts/: æœ€çµ‚åœ–è¡¨è¼¸å‡º
        - outputs/<session_id>/docs/: æœ€çµ‚æ–‡ä»¶è¼¸å‡º
    """
    logger = get_logger()
    logger.info("âœï¸ Phase 3: ç”Ÿæˆå…§å®¹...")
    
    phase_dir = session_dir / "phase3"
    ensure_dir(phase_dir)
    
    generated_charts = []
    generated_docs = []
    
    # ç”Ÿæˆåœ–è¡¨
    if planner_output.chart_todos:
        output_base = config.outputs_dir / session_id
        chart_loop = ChartLoop(
            log_dir=str(phase_dir / "charts"),
            output_dir=str(output_base / "charts")
        )
        
        total = len(planner_output.chart_todos)
        for i, todo in enumerate(planner_output.chart_todos):
            logger.op_progress(Operation.GENERATE, f"[{i+1}/{total}] åœ–è¡¨: {todo.title}")
            try:
                # è½‰æ› ChartTodo ç‚º ChartTask æ ¼å¼
                from models import ChartTask as ModelChartTask, ChartType
                
                chart_type_str = todo.chart_type.lower()
                try:
                    chart_type = ChartType(chart_type_str)
                except ValueError:
                    chart_type = ChartType.FLOWCHART
                
                model_task = ModelChartTask(
                    chart_type=chart_type,
                    title=todo.title,
                    description=todo.description,
                    instructions=todo.description,
                    suggested_files=todo.suggested_files,
                    suggested_participants=todo.suggested_participants,
                    questions_to_answer=todo.questions
                )
                
                result = chart_loop.run_from_task(task=model_task, project_path=str(project_path))
                generated_charts.append({
                    "success": result.success,
                    "title": todo.title,
                    "path": result.image_path,
                    "error": result.error
                })
                
                if not result.success:
                    logger.warning(f"âš  åœ–è¡¨å¤±æ•—: {todo.title} - {result.error}")
            except Exception as e:
                logger.error(f"âŒ åœ–è¡¨éŒ¯èª¤: {todo.title} - {e}")
                generated_charts.append({"success": False, "title": todo.title, "error": str(e)})
    
    # ç”Ÿæˆæ–‡æª”ï¼ˆä½¿ç”¨æ•´åˆå¾Œçš„ DocWriterï¼‰
    if planner_output.doc_todos:
        writer = DocWriter(project_path=str(project_path))
        output_base = config.outputs_dir / session_id
        docs_output_dir = output_base / "docs"
        ensure_dir(docs_output_dir)
        
        total = len(planner_output.doc_todos)
        for i, todo in enumerate(planner_output.doc_todos):
            logger.op_progress(Operation.GENERATE, f"[{i+1}/{total}] æ–‡ä»¶: {todo.title}")
            try:
                # ä½¿ç”¨æ•´åˆå¾Œçš„ execute_from_todo
                result = writer.execute_from_todo(
                    todo=todo,
                    report=project_structure,
                    project_path=str(project_path)
                )
                
                doc_filename = f"{todo.title.replace(' ', '_').replace('/', '_')}.md"
                doc_content = result.get("content", "")
                
                # é©—è­‰å…§å®¹ä¸ç‚ºç©º
                if not doc_content or not doc_content.strip():
                    raise ValueError(f"Generated empty content for: {todo.title}")
                
                output_path = docs_output_dir / doc_filename
                write_file(str(output_path), doc_content)
                
                generated_docs.append({
                    "success": True,
                    "title": todo.title,
                    "path": str(output_path)
                })
            except Exception as e:
                logger.error(f"âŒ æ–‡ä»¶éŒ¯èª¤: {todo.title} - {e}")
                generated_docs.append({"success": False, "title": todo.title, "error": str(e)})
    
    logger.finish_progress()
    
    # å„²å­˜çµæœ
    results = {"charts": generated_charts, "docs": generated_docs}
    with open(phase_dir / "results.json", "w", encoding="utf-8") as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    success_charts = len([c for c in generated_charts if c.get('success')])
    success_docs = len([d for d in generated_docs if d.get('success')])
    logger.info(f"   ç”Ÿæˆ {success_charts} å¼µåœ–è¡¨, {success_docs} ä»½æ–‡ä»¶")
    
    return generated_charts, generated_docs


def run_packer(generated_charts: list, generated_docs: list, project_name: str, session_dir: Path, session_id: str):
    """
    Phase 4: æ‰“åŒ…è¼¸å‡º
    
    Args:
        generated_charts: Phase 3 ç”Ÿæˆçš„åœ–è¡¨åˆ—è¡¨
        generated_docs: Phase 3 ç”Ÿæˆçš„æ–‡ä»¶åˆ—è¡¨
        project_name: å°ˆæ¡ˆåç¨±
        session_dir: Session ç›®éŒ„
        session_id: Session ID
    
    Output Files:
        - phase4/summary.json: æœ€çµ‚æ‘˜è¦
        - outputs/<session_id>/README.md: ç”Ÿæˆçš„ README
    """
    logger = get_logger()
    logger.info("ğŸ“¦ Phase 4: æ‰“åŒ…è¼¸å‡º...")
    
    phase_dir = session_dir / "phase4"
    ensure_dir(phase_dir)
    
    logger.op_progress(Operation.PACK, "ç”Ÿæˆ README...")
    
    # å»ºç«‹ README
    readme_lines = [
        f"# {project_name}",
        "",
        f"*Generated by Docu-chan on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*",
        ""
    ]
    
    if generated_charts:
        readme_lines.extend(["## Diagrams", ""])
        for chart in generated_charts:
            if chart.get("success") and chart.get("path"):
                name = Path(chart["path"]).name
                readme_lines.append(f"![{chart['title']}](charts/{name})")
                readme_lines.append("")
    
    if generated_docs:
        readme_lines.extend(["## Documentation", ""])
        for doc in generated_docs:
            if doc.get("success") and doc.get("path"):
                name = Path(doc["path"]).name
                readme_lines.append(f"- [{doc['title']}](docs/{name})")
        readme_lines.append("")
    
    readme_path = config.outputs_dir / session_id / "README.md"
    ensure_dir(readme_path.parent)
    write_file(readme_path, "\n".join(readme_lines))
    
    # ä¿å­˜æ‘˜è¦
    summary = {
        "charts": generated_charts,
        "docs": generated_docs,
        "readme": str(readme_path)
    }
    with open(phase_dir / "summary.json", "w", encoding="utf-8") as f:
        json.dump(summary, f, indent=2, ensure_ascii=False)
    
    logger.finish_progress()
    logger.info(f"   README: {readme_path}")


# ============================================================
# Main Pipeline
# ============================================================

def run_pipeline(
    project_path: Path,
    user_request: str,
    max_parallel: int = 5,
    resume_session: Optional[str] = None,
    from_phase: int = 1
):
    """åŸ·è¡Œå®Œæ•´ pipelineï¼ˆç°¡åŒ–æ¶æ§‹ï¼‰
    
    Args:
        project_path: å°ˆæ¡ˆè·¯å¾‘
        user_request: ä½¿ç”¨è€…è«‹æ±‚
        max_parallel: å¹³è¡Œè™•ç†æ•¸é‡
        resume_session: è¦æ¢å¾©çš„ session IDï¼ˆå¯é¸ï¼‰
        from_phase: å¾å“ªå€‹ phase é–‹å§‹ï¼ˆ1, 2, 3ï¼‰ï¼Œé è¨­ç‚º 1
    
    æµç¨‹ï¼š
    - Phase 1: å°ˆæ¡ˆåˆ†æ â†’ report.json
    - Phase 2: ä»»å‹™è¦åŠƒ â†’ planner_output.json
    - Phase 3: å…§å®¹ç”Ÿæˆ â†’ docs/, charts/
    - Phase 4: æ‰“åŒ…è¼¸å‡º â†’ README.md
    """
    # æ¢å¾©æ¨¡å¼æˆ–æ–°å»º session
    if resume_session:
        session_id = resume_session
        session_dir = config.logs_dir / session_id
        if not session_dir.exists():
            print(f"Error: Session not found: {session_id}")
            return False
    else:
        session_id = create_session_id()
        session_dir = config.logs_dir / session_id
        ensure_dir(session_dir)
    
    # åˆå§‹åŒ–æ—¥èªŒç³»çµ±ï¼ˆè¨­å®šæª”æ¡ˆæ—¥èªŒï¼‰
    logger = setup_logger(
        log_dir=config.logs_dir,
        session_id=session_id,
        show_thinking=True
    )
    
    logger.info(f"Session: {session_id}")
    logger.info(f"å°ˆæ¡ˆ: {project_path}")
    logger.info(f"è«‹æ±‚: {user_request}")
    if resume_session:
        logger.info(f"æ¢å¾©æ¨¡å¼: å¾ Phase {from_phase} é–‹å§‹")
    logger.info("=" * 50)
    
    try:
        # è¼‰å…¥ä¹‹å‰ session çš„è³‡æ–™ï¼ˆå¦‚æœæ˜¯æ¢å¾©æ¨¡å¼ï¼‰
        session_data = {}
        if resume_session and from_phase > 1:
            try:
                session_data = load_session_data(session_id, from_phase)
                logger.info(f"âœ“ å·²è¼‰å…¥ Session {session_id} çš„è³‡æ–™")
            except (FileNotFoundError, ValueError) as e:
                logger.error(f"ç„¡æ³•è¼‰å…¥ session è³‡æ–™: {e}")
                return False
        
        # Phase 1: å°ˆæ¡ˆåˆ†æ
        if from_phase <= 1:
            project_structure = run_analyzer(project_path, session_dir, max_parallel=max_parallel)
        else:
            project_structure = session_data.get("project_structure")
            logger.info("â­ è·³é Phase 1ï¼ˆä½¿ç”¨å·²æœ‰çš„åˆ†æçµæœï¼‰")
        
        # Phase 2: ä»»å‹™è¦åŠƒ
        if from_phase <= 2:
            planner_output = run_planner(project_structure, user_request, project_path, session_dir)
        else:
            planner_output = session_data.get("planner_output")
            logger.info("â­ è·³é Phase 2ï¼ˆä½¿ç”¨å·²æœ‰çš„è¦åŠƒçµæœï¼‰")
        
        # æª¢æŸ¥æ˜¯å¦æœ‰ä»»å‹™
        if not planner_output.doc_todos and not planner_output.chart_todos:
            logger.warning("âš  Planner æ²’æœ‰ç”¢ç”Ÿä»»ä½•ä»»å‹™ï¼Œè·³éå¾ŒçºŒéšæ®µ")
            return True
        
        # Phase 3: å…§å®¹ç”Ÿæˆï¼ˆæ•´åˆè¨­è¨ˆ+ç”Ÿæˆï¼‰
        charts, docs = run_generator(planner_output, project_structure, project_path, session_dir, session_id)
        
        # Phase 4: æ‰“åŒ…è¼¸å‡º
        run_packer(charts, docs, project_path.name, session_dir, session_id)
        
        logger.info("=" * 50)
        logger.info("âœ… Pipeline å®Œæˆ!")
        
        # é¡¯ç¤ºè¼¸å‡º
        logger = get_logger()
        output_base = config.outputs_dir / session_id
        logger.info("Outputs:")
        logger.info(f"  - README: {output_base / 'README.md'}")
        for c in charts:
            if c.get("success"):
                logger.info(f"  - Chart: {c['path']}")
        for d in docs:
            if d.get("success"):
                logger.info(f"  - Doc: {d['path']}")
        
        return True
        
    except Exception as e:
        import traceback
        logger = get_logger()
        logger.error(f"Pipeline failed: {e}")
        logger.debug(f"Pipeline error traceback:\n{traceback.format_exc()}")
        return False


def run_chart_only(description: str):
    """å–®ç¨ç”Ÿæˆåœ–è¡¨"""
    session_id = create_session_id()
    session_dir = config.logs_dir / session_id
    ensure_dir(session_dir)
    
    # åˆå§‹åŒ–æ—¥èªŒç³»çµ±
    logger = setup_logger(
        log_dir=config.logs_dir,
        session_id=session_id,
        show_thinking=True
    )
    
    logger.info("ğŸ¨ Chart-only æ¨¡å¼")
    logger.info(f"   è«‹æ±‚: {description}")
    
    chart_loop = ChartLoop(
        log_dir=str(session_dir / "chart"),
        output_dir=str(config.outputs_dir / "charts")
    )
    
    logger.op_progress(Operation.GENERATE, "ç”Ÿæˆåœ–è¡¨ä¸­...")
    result = chart_loop.run(description)
    logger.finish_progress()
    
    if result.success:
        logger.info(f"âœ… åœ–è¡¨å„²å­˜è‡³: {result.image_path}")
        return True
    else:
        logger.error(f"âŒ å¤±æ•—: {result.error}")
        return False


# ============================================================
# CLI
# ============================================================

def main():
    parser = argparse.ArgumentParser(
        description="Docu-chan - AI Documentation Generator"
    )
    parser.add_argument("project_path", nargs="?", help="Project path to analyze")
    parser.add_argument("-r", "--request", help="Documentation request")
    parser.add_argument("--chart", help="Generate single chart from description")
    parser.add_argument("-q", "--quiet", action="store_true", help="Quiet mode")
    parser.add_argument(
        "--parallel", "-p",
        type=int,
        default=5,
        help="Max parallel workers (default: 5, reduce if LLM errors occur)"
    )
    parser.add_argument(
        "--timeout",
        type=int,
        default=60,
        help="Worker timeout in seconds (default: 60)"
    )
    parser.add_argument(
        "--resume",
        type=str,
        help="Resume from a previous session ID (e.g., 20251229_224459)"
    )
    parser.add_argument(
        "--from-phase",
        type=int,
        default=1,
        choices=[1, 2, 3],
        help="Start from a specific phase (1, 2, or 3). Requires --resume."
    )
    
    args = parser.parse_args()
    
    # åˆå§‹åŒ–
    load_dotenv()
    try:
        load_config()
    except Exception as e:
        print(f"Error: {e}")
        return 1
    
    if not args.quiet:
        print("=" * 50)
        print("  Docu-chan v1.0.0")
        print("=" * 50)
    
    # Chart-only mode
    if args.chart:
        success = run_chart_only(args.chart)
        return 0 if success else 1
    
    # Validate resume arguments
    if args.from_phase > 1 and not args.resume:
        print("Error: --from-phase requires --resume to specify a session ID")
        return 1
    
    # Full pipeline
    if not args.project_path:
        parser.print_help()
        return 1
    
    project_path = Path(args.project_path).resolve()
    if not project_path.exists():
        print(f"Error: Path not found: {project_path}")
        return 1
    
    request = args.request or "Generate README and architecture documentation"
    
    success = run_pipeline(
        project_path,
        request,
        max_parallel=args.parallel,
        resume_session=args.resume,
        from_phase=args.from_phase
    )
    return 0 if success else 1


if __name__ == "__main__":
    sys.exit(main())
